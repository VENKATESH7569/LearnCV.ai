
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing & Analysis Toolkit\n",
    "## Computer Vision Fundamentals with OpenCV\n",
    "\n",
    "This notebook demonstrates the core image processing operations implemented in our Streamlit GUI application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Loading and Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_image():\n",
    "    \"\"\"Load a sample image for demonstration\"\"\"\n",
    "    url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Vd-Orig.png/256px-Vd-Orig.png'\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return np.array(img)\n",
    "\n",
    "def display_image_info(image, title=\"Image\"):\n",
    "    \"\"\"Display comprehensive image information\"\"\"\n",
    "    print(f\"{title} Information:\")\n",
    "    print(f\"Shape: {image.shape}\")\n",
    "    print(f\"Data Type: {image.dtype}\")\n",
    "    print(f\"Min Value: {image.min()}\")\n",
    "    print(f\"Max Value: {image.max()}\")\n",
    "    print(f\"Memory Size: {image.nbytes} bytes\")\n",
    "    print(\"\")\n",
    "\n",
    "original_img = load_sample_image()\n",
    "display_image_info(original_img, \"Original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Color Space Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "axes[0, 0].imshow(original_img)\n",
    "axes[0, 0].set_title('Original RGB')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "gray_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
    "axes[0, 1].imshow(gray_img, cmap='gray')\n",
    "axes[0, 1].set_title('Grayscale')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "hsv_img = cv2.cvtColor(original_img, cv2.COLOR_RGB2HSV)\n",
    "axes[0, 2].imshow(hsv_img)\n",
    "axes[0, 2].set_title('HSV')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(hsv_img[:,:,0], cmap='hsv')\n",
    "axes[1, 0].set_title('Hue Channel')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(hsv_img[:,:,1], cmap='gray')\n",
    "axes[1, 1].set_title('Saturation Channel')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(hsv_img[:,:,2], cmap='gray')\n",
    "axes[1, 2].set_title('Value Channel')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Geometric Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "height, width = original_img.shape[:2]\n",
    "center = (width // 2, height // 2)\n",
    "\n",
    "rotation_matrix = cv2.getRotationMatrix2D(center, 45, 1.0)\n",
    "rotated_img = cv2.warpAffine(original_img, rotation_matrix, (width, height))\n",
    "\n",
    "scaled_img = cv2.resize(original_img, None, fx=1.5, fy=1.5)\n",
    "\n",
    "translation_matrix = np.float32([[1, 0, 50], [0, 1, 30]])\n",
    "translated_img = cv2.warpAffine(original_img, translation_matrix, (width, height))\n",
    "\n",
    "pts1 = np.float32([[0, 0], [width, 0], [0, height]])\n",
    "pts2 = np.float32([[0, height*0.1], [width, height*0.1], [0, height]])\n",
    "affine_matrix = cv2.getAffineTransform(pts1, pts2)\n",
    "affine_img = cv2.warpAffine(original_img, affine_matrix, (width, height))\n",
    "\n",
    "axes[0, 0].imshow(rotated_img)\n",
    "axes[0, 0].set_title('Rotated (45Â°)')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(scaled_img)\n",
    "axes[0, 1].set_title('Scaled (1.5x)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(translated_img)\n",
    "axes[1, 0].set_title('Translated')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(affine_img)\n",
    "axes[1, 1].set_title('Affine Transform')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filtering Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "axes[0, 0].imshow(original_img)\n",
    "axes[0, 0].set_title('Original')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "gaussian_blur = cv2.GaussianBlur(original_img, (15, 15), 0)\n",
    "axes[0, 1].imshow(gaussian_blur)\n",
    "axes[0, 1].set_title('Gaussian Blur')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "median_blur = cv2.medianBlur(original_img, 15)\n",
    "axes[0, 2].imshow(median_blur)\n",
    "axes[0, 2].set_title('Median Blur')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "mean_blur = cv2.blur(original_img, (15, 15))\n",
    "axes[1, 0].imshow(mean_blur)\n",
    "axes[1, 0].set_title('Mean Filter')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "sharpened = cv2.filter2D(original_img, -1, kernel)\n",
    "axes[1, 1].imshow(sharpened)\n",
    "axes[1, 1].set_title('Sharpening Filter')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "gray = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
    "laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "laplacian = np.uint8(np.absolute(laplacian))\n",
    "axes[1, 2].imshow(laplacian, cmap='gray')\n",
    "axes[1, 2].set_title('Laplacian Filter')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "gray = cv2.cvtColor(original_img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "canny_edges = cv2.Canny(gray, 50, 150)\n",
    "axes[0, 0].imshow(canny_edges, cmap='gray')\n",
    "axes[0, 0].set_title('Canny Edge Detection')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "sobelx = np.uint8(np.absolute(sobelx))\n",
    "axes[0, 1].imshow(sobelx, cmap='gray')\n",
    "axes[0, 1].set_title('Sobel X')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "sobely = np.uint8(np.absolute(sobely))\n",
    "axes[1, 0].imshow(sobely, cmap='gray')\n",
    "axes[1, 0].set_title('Sobel Y')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "sobel_combined = np.sqrt(sobelx**2 + sobely**2)\n",
    "sobel_combined = np.uint8(sobel_combined)\n",
    "axes[1, 1].imshow(sobel_combined, cmap='gray')\n",
    "axes[1, 1].set_title('Sobel Combined')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Morphological Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "kernel = np.ones((5,5), np.uint8)\n",
    "\n",
    "dilated = cv2.dilate(original_img, kernel, iterations=1)\n",
    "axes[0, 0].imshow(dilated)\n",
    "axes[0, 0].set_title('Dilation')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "eroded = cv2.erode(original_img, kernel, iterations=1)\n",
    "axes[0, 1].imshow(eroded)\n",
    "axes[0, 1].set_title('Erosion')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "opened = cv2.morphologyEx(original_img, cv2.MORPH_OPEN, kernel)\n",
    "axes[1, 0].imshow(opened)\n",
    "axes[1, 0].set_title('Opening')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "closed = cv2.morphologyEx(original_img, cv2.MORPH_CLOSE, kernel)\n",
    "axes[1, 1].imshow(closed)\n",
    "axes[1, 1].set_title('Closing')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Image Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "axes[0, 0].imshow(original_img)\n",
    "axes[0, 0].set_title('Original')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "yuv = cv2.cvtColor(original_img, cv2.COLOR_RGB2YUV)\n",
    "yuv[:,:,0] = cv2.equalizeHist(yuv[:,:,0])\n",
    "hist_eq = cv2.cvtColor(yuv, cv2.COLOR_YUV2RGB)\n",
    "axes[0, 1].imshow(hist_eq)\n",
    "axes[0, 1].set_title('Histogram Equalization')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "lab = cv2.cvtColor(original_img, cv2.COLOR_RGB2LAB)\n",
    "lab[:,:,0] = clahe.apply(lab[:,:,0])\n",
    "clahe_img = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "axes[1, 0].imshow(clahe_img)\n",
    "axes[1, 0].set_title('CLAHE')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "contrast_stretched = cv2.convertScaleAbs(original_img, alpha=1.5, beta=30)\n",
    "axes[1, 1].imshow(contrast_stretched)\n",
    "axes[1, 1].set_title('Contrast Stretching')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Image Compression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_compression(image):\n",
    "    \"\"\"Analyze different compression formats and their file sizes\"\"\"\n",
    "    formats = ['PNG', 'JPG', 'BMP']\n",
    "    qualities = [95, 75, 50, 25]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for fmt in formats:\n",
    "        if fmt == 'JPG':\n",
    "            for quality in qualities:\n",
    "                encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
    "                success, encoded_img = cv2.imencode('.jpg', cv2.cvtColor(image, cv2.COLOR_RGB2BGR), encode_param)\n",
    "                if success:\n",
    "                    size_kb = len(encoded_img.tobytes()) / 1024\n",
    "                    results[f'{fmt}_Q{quality}'] = size_kb\n",
    "        else:\n",
    "            ext = '.png' if fmt == 'PNG' else '.bmp'\n",
    "            success, encoded_img = cv2.imencode(ext, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "            if success:\n",
    "                size_kb = len(encoded_img.tobytes()) / 1024\n",
    "                results[fmt] = size_kb\n",
    "    \n",
    "    return results\n",
    "\n",
    "compression_results = analyze_compression(original_img)\n",
    "\n",
    "print(\"Compression Analysis Results:\")\n",
    "print(\"============================\")\n",
    "for format_name, size in compression_results.items():\n",
    "    print(f\"{format_name}: {size:.2f} KB\")\n",
    "\n",
    "formats = list(compression_results.keys())\n",
    "sizes = list(compression_results.values())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(formats, sizes, color=['blue', 'red', 'red', 'red', 'red', 'green'])\n",
    "plt.title('File Size Comparison Across Different Formats')\n",
    "plt.xlabel('Format')\n",
    "plt.ylabel('Size (KB)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for bar, size in zip(bars, sizes):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{size:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Technical Concepts\n",
    "\n",
    "### CMOS vs CCD Sensors\n",
    "\n",
    "**CMOS (Complementary Metal-Oxide-Semiconductor):**\n",
    "- Lower power consumption\n",
    "- Faster readout speeds\n",
    "- On-chip processing capabilities\n",
    "- More cost-effective for mass production\n",
    "- Higher noise levels in low light\n",
    "\n",
    "**CCD (Charge-Coupled Device):**\n",
    "- Superior image quality\n",
    "- Better light sensitivity\n",
    "- Lower noise levels\n",
    "- Higher manufacturing costs\n",
    "- Slower processing speeds\n",
    "\n",
    "### Sampling and Quantization\n",
    "\n",
    "**Sampling** refers to the process of converting continuous spatial information into discrete pixels. The sampling rate determines the spatial resolution of the digital image.\n",
    "\n",
    "**Quantization** is the process of converting continuous intensity values into discrete digital values. Common quantization levels include:\n",
    "- 8-bit: 256 intensity levels (0-255)\n",
    "- 16-bit: 65,536 intensity levels\n",
    "- 32-bit: Over 4 billion intensity levels\n",
    "\n",
    "### Point Spread Function (PSF)\n",
    "\n",
    "The **Point Spread Function** describes how a point source of light is spread out in the imaging system. It characterizes:\n",
    "- Optical blur in the system\n",
    "- Spatial resolution limitations\n",
    "- Image degradation effects\n",
    "\n",
    "PSF is crucial for understanding image restoration and deconvolution techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "This notebook demonstrates fundamental image processing operations that form the backbone of computer vision applications:\n",
    "\n",
    "1. **Image Representation**: Understanding digital images as numerical arrays\n",
    "2. **Color Spaces**: RGB, HSV, Grayscale conversions for different processing needs\n",
    "3. **Geometric Transformations**: Rotation, scaling, translation for image manipulation\n",
    "4. **Filtering**: Smoothing and sharpening operations for noise reduction and enhancement\n",
    "5. **Edge Detection**: Identifying object boundaries and important features\n",
    "6. **Morphological Operations**: Shape-based processing for binary and grayscale images\n",
    "7. **Enhancement**: Improving image quality through histogram manipulation\n",
    "8. **Compression**: Understanding trade-offs between file size and image quality\n",
    "\n",
    "These operations serve as building blocks for more advanced computer vision tasks such as object detection, image segmentation, and pattern recognition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
